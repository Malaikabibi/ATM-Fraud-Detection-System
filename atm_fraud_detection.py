# -*- coding: utf-8 -*-
"""ATM Fraud Detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vipkGPOf589Ek0zjNIxMKjiN7xZ6prts
"""

# Import necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import folium
from geopy.geocoders import Nominatim
from datetime import datetime
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import IsolationForest
# Ignore warnings
import warnings
warnings.filterwarnings('ignore')

#Load the dataset
df = pd.read_csv('/content/bank_transactions_data_2 (1).csv')

# Print columns to verify what we have
print("Columns in dataset:", df.columns.tolist())

# Convert TransactionDate to datetime
df['TransactionDate'] = pd.to_datetime(df['TransactionDate'], errors='coerce')

# Security Checks
print("\n[Security Checks Running...]")

# Initialize all flags with False first
df['HighAmountFlag'] = False
df['GeoMissingFlag'] = False
df['RapidTransFlag'] = False
df['SuspiciousLogin'] = False
df['OddHourFlag'] = False
df['NegativePostBalanceFlag'] = False
df['LowPostBalanceFlag'] = False

# 1. High transaction amount warning
high_amount_threshold = 10000  # Define threshold as needed
if 'TransactionAmount' in df.columns:
    df['HighAmountFlag'] = df['TransactionAmount'] > high_amount_threshold
print("High Amount Transactions Detected:", df['HighAmountFlag'].sum())

# 2. Geolocation anomaly check (check if we have either coordinates or lat/long columns)
if 'Coordinates' in df.columns:
    df['GeoMissingFlag'] = df['Coordinates'].isnull()
elif 'Latitude' in df.columns and 'Longitude' in df.columns:
    df['GeoMissingFlag'] = df['Latitude'].isnull() | df['Longitude'].isnull()
else:
    print("No geolocation data columns found")
print("Transactions with missing geolocation data:", df['GeoMissingFlag'].sum())

# 3. Multiple rapid transactions from same customer within 5 minutes
if 'CustomerID' in df.columns and 'TransactionDate' in df.columns:
    df = df.sort_values(by=['CustomerID', 'TransactionDate'])
    df['TimeDelta'] = df.groupby('CustomerID')['TransactionDate'].diff().dt.total_seconds() / 60
    df['RapidTransFlag'] = df['TimeDelta'] <= 5
    print("Rapid consecutive transactions:", df['RapidTransFlag'].sum())

# 4. Suspicious login attempts > threshold (e.g., 3 attempts)
if 'LoginAttempts' in df.columns:
    login_attempt_threshold = 3
    df['SuspiciousLogin'] = df['LoginAttempts'] > login_attempt_threshold
    print("Suspicious login attempts:", df['SuspiciousLogin'].sum())

# 5. Transaction at odd hours (e.g., between 12 AM and 4 AM)
if 'TransactionDate' in df.columns:
    df['Hour'] = df['TransactionDate'].dt.hour
    df['OddHourFlag'] = df['Hour'].apply(lambda x: x < 4)
    print("Transactions at odd hours:", df['OddHourFlag'].sum())

# 6. Low balance after transaction
if 'AccountBalance' in df.columns and 'TransactionAmount' in df.columns:
    df['PostBalance'] = df['AccountBalance'] - df['TransactionAmount']
    df['NegativePostBalanceFlag'] = df['PostBalance'] <= 0
    df['LowPostBalanceFlag'] = df['PostBalance'] < 100
    print("Transactions causing negative or zero post-balance:", df['NegativePostBalanceFlag'].sum())
else:
    print("AccountBalance or TransactionAmount column missing, skipping post-balance check.")

# Fraud Scoring based on flags
df['FraudScore'] = df[['HighAmountFlag', 'GeoMissingFlag', 'RapidTransFlag',
                       'SuspiciousLogin', 'OddHourFlag', 'NegativePostBalanceFlag']].sum(axis=1)

# Step 1: Visualize each fraud flag
import seaborn as sns
import matplotlib.pyplot as plt

flag_cols = ['HighAmountFlag', 'GeoMissingFlag', 'RapidTransFlag',
             'SuspiciousLogin', 'OddHourFlag', 'NegativePostBalanceFlag']

plt.figure(figsize=(15, 8))
for i, col in enumerate(flag_cols):
    plt.subplot(2, 3, i+1)
    sns.countplot(x=df[col])
    plt.title(f'{col} Count')
plt.tight_layout()
plt.show()

# Step 2: Plot FraudScore distribution
plt.figure(figsize=(8, 4))
sns.histplot(df['FraudScore'], bins=range(df['FraudScore'].max() + 2), kde=False)
plt.title('Distribution of Fraud Scores')
plt.xlabel('Fraud Score')
plt.ylabel('Number of Transactions')
plt.show()

# Step 3: Add Likely Fraud flag
df['LikelyFraud'] = df['FraudScore'] >= 3  # you can change this threshold
print("Number of likely fraudulent transactions:", df['LikelyFraud'].sum())

# Basic Data Exploration
print("Shape of DataFrame:", df.shape)
print("Columns in DataFrame:", df.columns)
print("First 10 rows of DataFrame:\n", df.head(10))
print("Descriptive statistics:\n", df.describe(include='all'))
print("Info about DataFrame:\n", df.info())
print("Missing values per column:\n", df.isna().sum())
print("Duplicated rows count:", df.duplicated().sum())
print("Unique values per column:\n", df.nunique())
# Visualizing locations by transaction volume
all_locations = df['Location'].value_counts().head(43)
plt.figure(figsize=(10, 6))
sns.barplot(y=all_locations.index, x=all_locations.values, palette='viridis')
plt.title('All Locations by Transaction Volume', fontsize=19)
plt.xlabel('Transaction Volume', fontsize=17)
plt.ylabel('Location', fontsize=17)
plt.show()
# Violin plot of Transaction Amount
plt.figure(figsize=(10, 6))
sns.violinplot(y='TransactionAmount', data=df)
plt.title('Violin Plot of Transaction Amount', fontsize=20)
plt.ylabel('Transaction Amount', fontsize=18)
plt.show()
# Folium map for unique locations
unique_locations = df['Location'].unique()
location_coords = {}
geolocator = Nominatim(user_agent="location_mapper")
from geopy.geocoders import Nominatim
import time

# Initialize the geolocator
geolocator = Nominatim(user_agent="myApp")

# Initialize an empty dictionary to store coordinates
location_coords = {}

# List of unique locations (ensure this is defined)
unique_locations = ['New York', 'London', 'Tokyo']  # Example

# Get approximate coordinates for each unique location
for location in unique_locations:
    try:
        # Perform geocoding request
        loc = geolocator.geocode(location)

        if loc:
            location_coords[location] = (loc.latitude, loc.longitude)
        else:
            print(f"Coordinates not found for {location}")

    except Exception as e:
        # Handle any exceptions that occur during the geocoding process
        print(f"Error fetching coordinates for {location}: {e}")

    # Optional: Add a short delay to avoid hitting rate limits
    time.sleep(1)  # Pause for 1 second between requests to be courteous to the API

# Print the resulting coordinates for all locations
print(location_coords)
# Add coordinates to the DataFrame
df['Coordinates'] = df['Location'].map(location_coords)
# Initialize the Folium map centered at an approximate central point
initial_coords = list(location_coords.values())[0] if location_coords else [0, 0]
mymap = folium.Map(location=initial_coords, zoom_start=5)
import folium
import pandas as pd
from IPython.display import display

# Initialize the folium map (ensure you have an initial point for the map)
initial_coords = [37.7749, -122.4194]  # Example coordinates (San Francisco)
mymap = folium.Map(location=initial_coords, zoom_start=5)

# Assuming df has the 'Coordinates' column containing (latitude, longitude) pairs
for _, row in df.iterrows():
    if pd.notnull(row['Coordinates']) and isinstance(row['Coordinates'], (tuple, list)) and len(row['Coordinates']) == 2:
        folium.Marker(
            location=row['Coordinates'],
            popup=f"TransactionID: {row['TransactionID']}<br>Amount: ${row['TransactionAmount']}",
            tooltip=row['Location']
        ).add_to(mymap)

# Display the map directly in the notebook
display(mymap)
import folium
import pandas as pd
from IPython.display import display
from google.colab import files  # For downloading the file from Colab

# Initialize the folium map (ensure you have an initial point for the map)
initial_coords = [37.7749, -122.4194]  # Example coordinates (San Francisco)
mymap = folium.Map(location=initial_coords, zoom_start=5)

# Assuming df has the 'Coordinates' column containing (latitude, longitude) pairs
for _, row in df.iterrows():
    if pd.notnull(row['Coordinates']) and isinstance(row['Coordinates'], (tuple, list)) and len(row['Coordinates']) == 2:
        folium.Marker(
            location=row['Coordinates'],
            popup=f"TransactionID: {row['TransactionID']}<br>Amount: ${row['TransactionAmount']}",
            tooltip=row['Location']
        ).add_to(mymap)

# Save the map to the '/content/' directory in Colab
mymap.save('/content/transaction_map.html')

# Display the map in the notebook
display(mymap)

# Optionally, if you want to download the map file from Colab
files.download('/content/transaction_map.html')
# Histplot of Transaction Amount
plt.figure(figsize=(10, 6))
sns.histplot(data=df, x="TransactionAmount", kde=True)
plt.xlabel("Transaction Amount", fontsize=20)
plt.ylabel("Count", fontsize=20)
plt.title("Histplot of Transaction Amount", fontsize=23)
plt.show()
# Box plot for Transaction Amount
plt.figure(figsize=(10, 6))
sns.boxplot(x='TransactionAmount', data=df, color='gold')
plt.title('Box Plot of Transaction Amount', fontsize=20)
plt.xlabel('Transaction Amount', fontsize=18)
plt.show()
# Transaction Type Distribution
transaction_counts = df['TransactionType'].value_counts()
plt.figure(figsize=(10, 6))
plt.pie(transaction_counts, labels=transaction_counts.index, autopct='%2.2f%%')
plt.title('Overall Distribution of Transaction Type', fontsize=20)
plt.show()
# Location counts
location_counts = df['Location'].value_counts()
plt.figure(figsize=(10, 6))
plt.bar(location_counts.index, location_counts.values, color='turquoise', edgecolor='black')
plt.xticks(location_counts.index, location_counts.index, rotation=90, fontsize=14)
plt.xlabel('Location', fontsize=19)
plt.ylabel('Total Number of Values in the Dataset', fontsize=19)
plt.title('Overall Distribution of Transaction Location', fontsize=23)
plt.show()
# Date processing and Transaction Count by Day
df['TransactionDate'] = pd.to_datetime(df['TransactionDate'])
df['TransactionDay'] = df['TransactionDate'].dt.date
daily_counts = df.groupby('TransactionDay').size()
plt.figure(figsize=(15, 8))
daily_counts.plot(title='Daily Transaction Counts', color='blue', linewidth=2)
plt.xlabel('Date')
plt.ylabel('Transaction Count')
plt.show()
# Transaction Count by Day of the Week
df['DayOfWeek'] = df['TransactionDate'].dt.day_name()
plt.figure(figsize=(10, 6))
sns.countplot(data=df, x='DayOfWeek', order=['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'])
plt.title('Count Plot of Transactions by Day of the Week', fontsize=19)
plt.xlabel('Day Of The Week', fontsize=17)
plt.ylabel('Count', fontsize=17)
plt.show()
# Hours and Transaction Frequency
df['Hour'] = df['TransactionDate'].dt.hour
plt.figure(figsize=(12, 6))
sns.histplot(data=df, x='Hour', kde=True, palette='plasma')
plt.title('Transaction Frequency by Hour of the Day', fontsize=19)
plt.xlabel('Hour of the Day', fontsize=17)
plt.ylabel('Transaction Frequency', fontsize=17)
plt.show()
# Time Gap Analysis
df['PreviousTransactionDate'] = pd.to_datetime(df['PreviousTransactionDate'])
df['Time_Gap'] = (df['TransactionDate'] - df['PreviousTransactionDate'])
df['TimeGap'] = (df['TransactionDate'] - df['PreviousTransactionDate']).dt.total_seconds() / 60  # converting to minutes
plt.figure(figsize=(10, 6))
sns.histplot(data=df, x='TimeGap', bins=30, palette='mako', kde=True, hue='TransactionType', multiple='stack')
plt.title('Distribution of Time Gap Between Transactions (in Minutes)')
plt.xlabel('Time Gap (in Minutes)')
plt.ylabel('Frequency')
plt.show()
# Customer Age Distribution
plt.figure(figsize=(10, 6))
sns.histplot(data=df, x='CustomerAge', kde=True, color='orange')
plt.title('Overall Distribution of Customer Age', fontsize=23)
plt.xlabel('Customer Age', fontsize=19)
plt.ylabel('Count', fontsize=19)
plt.show()
# Age Distribution by Transaction Type
plt.figure(figsize=(10, 6))
sns.histplot(data=df, x='CustomerAge', hue='TransactionType', multiple='stack', palette='viridis', kde=True)
plt.title('Customer Age Distribution by Transaction Type')
plt.xlabel('Customer Age')
plt.ylabel('Frequency')
plt.show()
# Merchant Analysis
merchant_counts_top10 = df['MerchantID'].value_counts().head(10)
plt.figure(figsize=(10, 5))
merchant_counts_top10.plot(kind='bar', color='teal')
plt.title("Top 10 Most Frequent Merchants")
plt.xlabel("MerchantID")
plt.ylabel("Transaction Count")
plt.show()
# Most Frequent Merchants
plt.figure(figsize=(10, 6))
sns.barplot(y=df['MerchantID'].value_counts().head(60).index, x=df['MerchantID'].value_counts().head(60).values, palette='plasma')
plt.title('Top 30 Most Frequent Merchants', fontsize=19)
plt.xlabel('Counts', fontsize=17)
plt.ylabel('Merchant ID', fontsize=17)
plt.yticks(fontsize=7.5)
plt.show()
# Customer Occupation Distribution
customer_occupation_counts = df['CustomerOccupation'].value_counts()
plt.figure(figsize=(10, 6))
colors = ['aqua', 'orange', 'gold', 'yellowgreen']
plt.bar(customer_occupation_counts.index, customer_occupation_counts.values, color=colors[:len(customer_occupation_counts)], edgecolor='black')
plt.xlabel('Customer Occupation', fontsize=17)
plt.ylabel('Total Number', fontsize=17)
plt.title('Overall Distribution of Customer Occupation', fontsize=20)
plt.show()
# Customer Occupation Distribution Summary
occupation_counts = df['CustomerOccupation'].value_counts()
plt.figure(figsize=(10, 6))
index_values = [occupation_counts.get('Student', 0), occupation_counts.get('Doctor', 0),
                occupation_counts.get('Engineer', 0), occupation_counts.get('Retired', 0)]
index_labels = ['Student', 'Doctor', 'Engineer', 'Retired']
plt.pie(index_values, labels=index_labels, autopct='%2.2f%%')
plt.title('Overall Distribution of Transactions by Customer Occupation', fontsize=21)
plt.show()
# Account Balance Density Plot
plt.figure(figsize=(10, 6))
sns.kdeplot(data=df, x='AccountBalance', shade=True, color='navy')
plt.xlabel('Account Balance', fontsize=17)
plt.ylabel('Density', fontsize=17)
plt.title('Density Plot of Account Balance of the Customer', fontsize=21)
plt.show()
# Channel Distribution By Transaction Type
plt.figure(figsize=(10, 6))
sns.countplot(data=df, x='Channel', hue='TransactionType', palette='mako')
plt.xlabel('Channel', fontsize=17)
plt.ylabel('Count', fontsize=17)
plt.title('Count Plot of Channel', fontsize=19)
plt.show()
# Overall Distribution of Transaction Types
plt.figure(figsize=(10, 6))
sns.countplot(data=df, x='TransactionType', palette='muted')
plt.title('Overall Distribution of Transaction Type', fontsize=19)
plt.show()
# Box Plot of Transaction Amount by Transaction Type
plt.figure(figsize=(10, 6))
sns.boxplot(data=df, x='TransactionAmount', y='TransactionType', palette='plasma')
plt.xscale('log')  # Log scale to show outliers
plt.title('Box Plot of Transaction Amount by Transaction Type', fontsize=19)
plt.show()
# Transaction Amount by Age Group
df['AgeGroup'] = pd.cut(df['CustomerAge'], bins=[0, 25, 35, 50, 100], labels=['18-25', '26-35', '36-50', '51+'])
plt.figure(figsize=(10, 6))
sns.boxplot(data=df, x='AgeGroup', y='TransactionAmount', palette='viridis')
plt.title('Transaction Amount by Age Group')
plt.show()
# Joint Plot of Transaction Amount and Customer Age
plt.figure(figsize=(10, 6))
sns.jointplot(x='CustomerAge', y='TransactionAmount', data=df, kind='hex', color='aqua')
plt.suptitle('Joint Plot of Transaction Amount and Customer Age', y=1.02, fontsize=19)
plt.show()
# Regression Plot of Transaction Duration and Amount
plt.figure(figsize=(10, 6))
sns.regplot(x='TransactionDuration', y='TransactionAmount', data=df, scatter_kws={'alpha': 0.5}, line_kws={'color': 'red'})
plt.title('Transaction Amount vs. Transaction Duration', fontsize=19)
plt.xlabel('Transaction Duration', fontsize=17)
plt.ylabel('Transaction Amount', fontsize=17)
plt.show()
# Scatter plot of Account Balance vs. Transaction Duration
plt.figure(figsize=(10, 6))
sns.scatterplot(y='AccountBalance', x='TransactionDuration', data=df)
plt.xlabel('Transaction Duration', fontsize=17)
plt.ylabel('Account Balance', fontsize=17)
plt.title('Account Balance vs. Transaction Duration', fontsize=19)
plt.show()
# Scatter plot of Transaction Amount vs. Account Balance
plt.figure(figsize=(10, 6))
sns.scatterplot(data=df, x='AccountBalance', y='TransactionAmount', hue='TransactionType', alpha=0.6)
plt.title('Transaction Amount vs. Account Balance')
plt.xlabel('Account Balance')
plt.ylabel('Transaction Amount')
plt.legend(title='TransactionType')
plt.show()
# Log-scaled scatter plot of Account Balance vs. Transaction Amount
plt.figure(figsize=(10, 6))
sns.scatterplot(data=df, x='TransactionAmount', y='AccountBalance', hue='TransactionType', palette='viridis')
plt.xscale('log')  # Log Scale for Anomaly Detection
plt.yscale('log')  # Log Scale for Anomaly Detection
plt.title("Account Balance vs Transaction Amount")
plt.xlabel("Transaction Amount")
plt.ylabel("Account Balance")
plt.show()
# Regression plot of Account Balance vs. Transaction Duration
plt.figure(figsize=(10, 6))
sns.regplot(y='AccountBalance', x='TransactionDuration', data=df, color='red', scatter_kws={'alpha': 0.5}, line_kws={'color': 'purple'})
plt.xlabel('Transaction Duration', fontsize=17)
plt.ylabel('Account Balance', fontsize=17)
plt.title('Account Balance vs. Transaction Duration', fontsize=19)
plt.show()
# Hexagonal Binning Plot of Customer Age vs. Account Balance
plt.figure(figsize=(10, 6))
df.plot(kind='hexbin', x='CustomerAge', y='AccountBalance', gridsize=20)
plt.title('Hexagonal Binning Plot', fontsize=19)
plt.xlabel('Customer Age', fontsize=17)
plt.ylabel('Account Balance', fontsize=17)
plt.show()
# Regression plot of Account Balance vs. Customer Age
plt.figure(figsize=(10, 6))
sns.regplot(x='CustomerAge', y='AccountBalance', data=df, scatter_kws={'alpha': 0.5}, line_kws={'color': 'red'})
plt.title('Regression Plot of AccountBalance vs. CustomerAge', fontsize=19)
plt.xlabel('Customer Age', fontsize=17)
plt.ylabel('Account Balance', fontsize=17)
plt.show()
# Joint Plot of Login Attempts vs. Transaction Duration
plt.figure(figsize=(8, 6))
sns.jointplot(y='LoginAttempts', x='TransactionDuration', data=df, color='teal')
plt.xlabel('Transaction Duration', fontsize=11)
plt.ylabel('Login Attempts', fontsize=11)
plt.suptitle('Joint Plot of Login Attempts and Transaction Duration', y=1.02, fontsize=19)
plt.show()
# Regression Plot of Transaction Amount vs. Login Attempts
plt.figure(figsize=(10, 6))
sns.regplot(x='TransactionAmount', y='LoginAttempts', data=df, scatter_kws={'alpha': 0.5}, line_kws={'color': 'red'})
plt.xlabel('Transaction Amount', fontsize=17)
plt.ylabel('Login Attempts', fontsize=17)
plt.title('Login Attempts vs. Transaction Amount', fontsize=19)
plt.show()
# Joint Plot of Transaction Amount vs. Login Attempts
plt.figure(figsize=(10, 6))
sns.jointplot(x='TransactionAmount', y='LoginAttempts', data=df, color='purple')
plt.suptitle('Joint Plot of Login Attempts vs. Transaction Amount', y=1.02)
plt.show()
# Regression Plot of Account Balance vs. Login Attempts
plt.figure(figsize=(10, 6))
sns.regplot(x='AccountBalance', y='LoginAttempts', data=df, scatter_kws={'alpha': 0.5}, line_kws={'color': 'red'})
plt.xlabel('Account Balance', fontsize=17)
plt.ylabel('Login Attempts', fontsize=17)
plt.title('Login Attempts vs. Account Balance', fontsize=19)
plt.show()
# Joint Plot of Login Attempts vs. Account Balance
plt.figure(figsize=(10, 6))
sns.jointplot(x='AccountBalance', y='LoginAttempts', data=df, color='orange')
# Scatter Plot of Transaction Duration vs. Transaction Amount
plt.figure(figsize=(10, 6))
sns.scatterplot(x='TransactionDuration', y='TransactionAmount', data=df, hue='TransactionType', palette='magma')
plt.show()
# Correlation Heatmap
numeric_columns = df.select_dtypes(include=np.number).columns
correlation_matrix = df[numeric_columns].corr()
plt.figure(figsize=(10, 6))
sns.heatmap(correlation_matrix, annot=True, cmap='Blues', fmt='.2f')
plt.xticks(rotation=90, fontsize=17)
plt.yticks(fontsize=17)
plt.title('Correlation Heatmap', fontsize=20)
plt.show()
# Anomaly Detection Analysis
device_variations = df.groupby('AccountID')['DeviceID'].nunique()
ip_variations = df.groupby('AccountID')['IP Address'].nunique()
plt.figure(figsize=(14, 6))
plt.subplot(1, 2, 1)
sns.histplot(device_variations, bins=20, color='red', kde=True)
plt.title("Number of Device Variations per Account")
plt.xlabel("Unique Devices")
plt.subplot(1, 2, 2)
sns.histplot(ip_variations, bins=20, color='purple', kde=True)
plt.title("Number of IP Address Variations per Account")
plt.xlabel("Unique IP Addresses")
plt.show()
# KMeans Clustering
X = df[['TransactionAmount', 'CustomerAge']]

# Impute missing values using the mean for example
X = X.fillna(X.mean())

#Alternatively, you can drop rows with missing values:
#X = X.dropna()


scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
kmeans = KMeans(n_clusters=3, random_state=42)
kmeans_labels = kmeans.fit_predict(X_scaled)
df['KMeans_Cluster'] = kmeans_labels
distances = np.linalg.norm(X_scaled - kmeans.cluster_centers_[kmeans_labels], axis=1)
threshold = np.percentile(distances, 95)
df['Potential_Fraud'] = distances > threshold
frauds = df[df['Potential_Fraud']]
# KMeans Clustering Plot
plt.figure(figsize=(10, 6))
sns.scatterplot(x=X_scaled[:, 0], y=X_scaled[:, 1], hue=kmeans_labels, palette='viridis', s=60, alpha=0.5)
plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=200, c='red', label='Centroids')
plt.scatter(X_scaled[distances > threshold, 0], X_scaled[distances > threshold, 1],
            color='black', s=50, label='Potential Frauds', marker='X')
plt.title('K-means Clustering with Potential Frauds Highlighted')
plt.xlabel('Scaled Amount')
plt.ylabel('Scaled Age')
plt.legend()
plt.show()
print(f"Number of potential frauds detected: {len(frauds)}")
# DBSCAN Clustering
dbscan = DBSCAN(eps=0.3, min_samples=5)
dbscan_labels = dbscan.fit_predict(X_scaled)
df['DBSCAN_Cluster'] = dbscan_labels
label_mapping = {-1: 'Fraud', 0: 'Normal', 1: 'Suspicious Group 1', 2: 'Suspicious Group 2'}
df['DBSCAN_Cluster'] = df['DBSCAN_Cluster'].map(label_mapping)
# DBSCAN Clustering Plot
plt.figure(figsize=(10, 6))
sns.scatterplot(x=X_scaled[:, 0], y=X_scaled[:, 1], hue=df['DBSCAN_Cluster'], palette='coolwarm', s=60)
plt.title('DBSCAN Clustering on Transactions')
plt.xlabel('Scaled Amount')
plt.ylabel('Scaled Age')
plt.legend(title='Cluster')
plt.show()
from sklearn.cluster import AgglomerativeClustering

# Hierarchical Clustering with the correct parameters
hierarchical = AgglomerativeClustering(n_clusters=3, linkage='ward')  # No 'affinity' argument when linkage='ward'
hierarchical_labels = hierarchical.fit_predict(X_scaled)

# Assigning the cluster labels to the DataFrame
df['Hierarchical_Cluster'] = hierarchical_labels

# Mapping cluster labels to meaningful categories
cluster_mapping = {0: 'Normal', 1: 'High Amount', 2: 'Older Age Group'}
df['Hierarchical_Cluster'] = df['Hierarchical_Cluster'].map(cluster_mapping)
# Hierarchical Clustering Plot
plt.figure(figsize=(10, 6))
sns.scatterplot(x=X_scaled[:, 0], y=X_scaled[:, 1], hue=df['Hierarchical_Cluster'], palette='Set2', s=60)
plt.title('Hierarchical Clustering on Transactions')
plt.xlabel('Scaled Amount')
plt.ylabel('Scaled Age')
plt.legend(title='Cluster')
plt.show()
# Isolation Forest for Anomaly Detection
iso_forest = IsolationForest(contamination=0.01, random_state=42)
outlier_pred = iso_forest.fit_predict(X_scaled)
outlier_mapping = {1: 'Normal', -1: 'Potential Fraud'}
df['Outlier_Prediction'] = pd.Series(outlier_pred).map(outlier_mapping)
# Isolation Forest Plot
plt.figure(figsize=(10, 6))
sns.scatterplot(x=X_scaled[:, 0], y=X_scaled[:, 1], hue=df['Outlier_Prediction'], palette='coolwarm', s=60)
plt.title('Outlier Detection on Transactions')
plt.xlabel('Scaled Amount')
plt.ylabel('Scaled Age')
plt.legend(title='Outlier Prediction')
plt.show()